ai-podcast-clipper-local/ai-podcast-clipper-local-backend/.venv


sudo tar -xzf components_env.tar.gz -C /
sudo tar -xzf project_models.tar.gz -C /


git clone https://github.com/ggerganov/llama.cpp.git /workspace/llama.cpp
LLAMA_CPP_DIR=/workspace/llama.cpp ./setup_local_services.sh start


LLAMA_CPP_DIR=/workspace/llama.cpp ./setup_local_services.sh start



You donâ€™t need an active env first. The components_env.tar.gz already contains the full backend .venv and the service binaries. On a new instance, you just extract and run.

Quick start on a new instance (assumes /workspace exists):

# 1) Extract packages
tar -xzf /workspace/packages/components_env.tar.gz -C /
tar -xzf /workspace/packages/project_models.tar.gz -C /

# 2) Ensure binaries are on PATH (optional)
export PATH="/root/bin:$PATH"

# 3) Start services
/workspace/ai-podcast-clipper-local/scripts/setup_local_services.sh start
Notes:

The venv is already inside the repo at ai-podcast-clipper-local/ai-podcast-clipper-local-backend/.venv.
Models are already in /workspace/models and /workspace/models/WhisperX is present (symlink), so WHISPER_MODEL_DIR should work out of the box.
If the GPU/driver/CUDA version differs, you may still need to reinstall the torch stack (torch==2.8.0, torchaudio==2.8.0, torchvision==0.23.0).
Want me to write a small bootstrap.sh script to bundle these steps?