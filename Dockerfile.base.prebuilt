# Dockerfile.base.prebuilt
# Runtime base image route: do not compile llama.cpp in this build.
# Reuse llama artifacts from a prebuilt base image and keep vast.ai as runtime base.
ARG LLAMA_ARTIFACT_IMAGE=ghcr.io/jianshelu/ledge-repo-base:llama-src-latest
ARG VAST_RUNTIME_IMAGE=vastai/base-image:cuda-12.4.1-auto
ARG TORCH_VERSION=2.6.0+cu124
ARG TORCH_INDEX_URL=https://download.pytorch.org/whl/cu124

FROM ${LLAMA_ARTIFACT_IMAGE} AS llama_artifacts
FROM ${VAST_RUNTIME_IMAGE}

ARG VAST_RUNTIME_IMAGE=vastai/base-image:cuda-12.4.1-auto

WORKDIR /workspace

# Reuse compiled llama-server artifacts from external image.
COPY --from=llama_artifacts /app/ /app/
RUN test -x /app/llama-server \
 && ln -sf /app/llama-server /usr/local/bin/llama-server

# Install system dependencies (+ supervisor)
# Vast.ai base can already carry a supervisord conffile; force non-interactive dpkg behavior.
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
    python3 python3-pip python3-venv ffmpeg curl wget git ca-certificates gnupg supervisor \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.instance.txt /tmp/requirements.instance.txt

RUN set -eux; \
  test -n "${TORCH_VERSION}"; \
  test -n "${TORCH_INDEX_URL}"; \
  test -s /tmp/requirements.instance.txt; \
  tr -d '\r' < /tmp/requirements.instance.txt > /tmp/requirements.instance.clean.txt; \
  if grep -nE '^[[:space:]]*torch==[[:space:]]*$|^[[:space:]]*pip/[[:space:]]*$' /tmp/requirements.instance.clean.txt; then exit 1; fi; \
  pip3 install --upgrade pip; \
  pip3 install --no-cache-dir -r /tmp/requirements.instance.clean.txt; \
  if ! python3 -c "import torch" >/dev/null 2>&1; then \
    pip3 install --no-cache-dir --index-url "${TORCH_INDEX_URL}" "torch==${TORCH_VERSION}"; \
  fi; \
  python3 -c "import torch; print('torch available:', torch.__version__, 'cuda:', torch.version.cuda)"; \
  rm -f /tmp/requirements.instance.txt /tmp/requirements.instance.clean.txt
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
    -o Dpkg::Options::="--force-confdef" \
    -o Dpkg::Options::="--force-confold" \
    python3 \
    python3-pip \
    python3-venv \
    ffmpeg \
    curl \
    wget \
    git \
    ca-certificates \
    gnupg \
    supervisor \
    && rm -rf /var/lib/apt/lists/*

# Install Node.js 20 (NodeSource)
RUN curl -fsSL https://deb.nodesource.com/setup_20.x | bash - \
 && apt-get update \
 && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
    -o Dpkg::Options::="--force-confdef" \
    -o Dpkg::Options::="--force-confold" \
    nodejs \
 && rm -rf /var/lib/apt/lists/*

# Default model/cache paths
ENV LLM_MODEL_PATH=/workspace/packages/models/llm \
    XDG_CACHE_HOME=/workspace/.cache

RUN mkdir -p /workspace/packages/models/llm /workspace/.cache

# Install instance runtime Python dependencies in base image
COPY requirements.instance.txt /tmp/requirements.instance.txt
RUN pip3 install --upgrade pip \
 && pip3 install --no-cache-dir -r /tmp/requirements.instance.txt \
 && if ! python3 -c "import torch" >/dev/null 2>&1; then \
      pip3 install --no-cache-dir --index-url "${TORCH_INDEX_URL}" "torch==${TORCH_VERSION}"; \
    fi \
 && python3 -c "import torch; print('torch available:', torch.__version__, 'cuda:', torch.version.cuda)" \
 && rm -f /tmp/requirements.instance.txt \
 && rm -rf /root/.cache/pip \
 && (find /usr/local/lib/python3.10/dist-packages -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true) \
 && (find /usr/local/lib/python3.10/dist-packages -type f -name "*.pyc" -delete 2>/dev/null || true) \
 && (find /usr/local/lib/python3.10/dist-packages -type f -name "*.pyo" -delete 2>/dev/null || true)

# Supervisor config + llama wrapper
COPY scripts/supervisord.conf /etc/supervisor/supervisord.conf
COPY scripts/start-llama.sh /usr/local/bin/start-llama.sh
COPY scripts/with_compute_env.sh /workspace/scripts/with_compute_env.sh
RUN chmod +x /usr/local/bin/start-llama.sh /workspace/scripts/with_compute_env.sh
