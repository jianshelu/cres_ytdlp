# Dockerfile.base (supervisord-enabled, single-container multi-service)
# Base image includes CUDA-enabled llama-server
ARG BASE_IMAGE=ghcr.io/ggml-org/llama.cpp:server-cuda
FROM ${BASE_IMAGE}

WORKDIR /workspace

# Install system dependencies (+ supervisor)
# Retry apt operations to survive transient mirror sync 404s.
RUN set -eux; \
    ok=0; \
    for i in 1 2 3 4 5; do \
      apt-get update && \
      apt-get install -y --no-install-recommends --fix-missing \
        python3 \
        python3-pip \
        python3-venv \
        ffmpeg \
        curl \
        wget \
        git \
        ca-certificates \
        gnupg \
        supervisor && { ok=1; } && \
      break; \
      echo "apt install attempt $i failed, retrying..."; \
      rm -rf /var/lib/apt/lists/*; \
      sleep 5; \
    done; \
    test "$ok" = "1"; \
    rm -rf /var/lib/apt/lists/*

# Install Node.js 20 (NodeSource)
RUN set -eux; \
    curl -fsSL https://deb.nodesource.com/setup_20.x | bash -; \
    ok=0; \
    for i in 1 2 3 4 5; do \
      apt-get update && \
      apt-get install -y --no-install-recommends --fix-missing nodejs && { ok=1; } && \
      break; \
      echo "nodejs install attempt $i failed, retrying..."; \
      rm -rf /var/lib/apt/lists/*; \
      sleep 5; \
    done; \
    test "$ok" = "1"; \
    rm -rf /var/lib/apt/lists/*

# Default model/cache paths
ENV LLM_MODEL_PATH=/workspace/packages/models/llm \
    XDG_CACHE_HOME=/workspace/.cache

RUN mkdir -p /workspace/packages/models/llm /workspace/.cache

# Supervisor config + llama wrapper
COPY scripts/supervisord.conf /etc/supervisor/supervisord.conf
COPY scripts/start-llama.sh /usr/local/bin/start-llama.sh
RUN chmod +x /usr/local/bin/start-llama.sh
