# Dockerfile.base (supervisord-enabled, single-container multi-service)
# Runtime base is Vast.ai image; llama runtime binaries are copied from ggml image.
# NOTE: "@vastai-automatic-tag" works in Vast template APIs, not in Docker pulls.
ARG BASE_IMAGE=vastai/base-image:cuda-12.4.1-auto
FROM ghcr.io/ggml-org/llama.cpp:server-cuda AS llama_runtime

FROM ${BASE_IMAGE}

WORKDIR /workspace

# Install minimal runtime dependencies (+ supervisor)
# Retry apt operations to survive transient mirror sync 404s.
RUN set -eux; \
    ok=0; \
    for i in 1 2 3 4 5; do \
      apt-get update && \
      apt-get install -y --no-install-recommends --fix-missing \
        python3 \
        python3-pip \
        ffmpeg \
        ca-certificates \
        supervisor && { ok=1; } && \
      break; \
      echo "apt install attempt $i failed, retrying..."; \
      rm -rf /var/lib/apt/lists/*; \
      sleep 5; \
    done; \
    test "$ok" = "1"; \
    rm -rf /var/lib/apt/lists/*

# Default model/cache paths
ENV LLM_MODEL_PATH=/workspace/packages/models/llm \
    XDG_CACHE_HOME=/workspace/.cache

RUN mkdir -p /workspace/packages/models/llm /workspace/.cache

# Copy llama runtime payload (server binary + CUDA-linked runtime libs) at build time.
# This avoids runtime compilation and keeps startup deterministic.
RUN mkdir -p /opt/llama
COPY --from=llama_runtime /app/ /opt/llama/
RUN if [ -f /opt/llama/llama-server ]; then ln -sf /opt/llama/llama-server /usr/local/bin/llama-server; fi \
 && if [ -f /opt/llama/llama-cli ]; then ln -sf /opt/llama/llama-cli /usr/local/bin/llama-cli; fi

# Supervisor config + llama wrapper
COPY scripts/supervisord.conf /etc/supervisor/supervisord.conf
COPY scripts/start-llama.sh /usr/local/bin/start-llama.sh
RUN chmod +x /usr/local/bin/start-llama.sh
